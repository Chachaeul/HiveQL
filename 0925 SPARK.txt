start-master.sh

윈도우 > http://192.168.10.1:8080  접속

start-slave.sh spark://master:7077 -m 512m -c 1
jps						// worker 3개 확인


https://zeppelin.apache.org/ 에서 > 다운로드 >  zeppelin : 0.7.3 버전 다운로드

http://spark.apache.org/ 에서 다운로드 > archived releases > spark-2.1.3 버전 >  spark-2.1.3-bin-hadoop2.7.tgz             2018-06-26 17:06  185M  받기

https://community.cloud.databricks.com 가입, 로그인 >  왼쪽 clusters에서 new cluster 만들기
	Cluster Name : mycluster, 이외 기본 설정

만들어지면 cluster 내 > 상단에 Libraries 클릭 > Install now > maven 탭 > graphframes 검색 > select > install
또 Install now > PyPl 탭 > pandas 설치

메인화면 > new Notebook > 이름 : mynote 이외 기본설정 만들기 > 
sc 		// ctrl+ enter 로 실행
spark 


zeppelin 합니다>>리눅스로 root폴더로 전송	
tar zxvf zeppelin...
mv zeppelin... zeppelin
             

gedit /etc/profile
--------------------
export ZEPPELIN_HOME=/root/zeppelin
export PATH=$PATH:$ZEPPELIN_HOME/bin
--------------------
                 
 source /etc/profile
reboot
				  

cd $ZEPPELIN_HOME/conf
cp zeppelin-site.xml.template zeppelin-site.xml
				


터미널 창 하나 더 열어서
ifconfig 		//enp0s8의 inet 확인 

gedit zeppelin-site.xml
------------------------------
zeppeling.server.addr 있는 곳에 value >>  0.0.0.0 을 enp0s8의 inet으로 바꾸기
zeppeling.server.port 있는 곳에 value >> 7777 로 바꾸기

저장 후 닫기

cp zeppelin-env.sh.template zeppelin-env.sh	
gedit zeppelin-env.sh
-----------------------------
export JAVA_HOME=$JAVA_HOME
export MASTER=spark://master:7077
export SPARK_HOME=/root/spark



start-master.sh
start-slave.sh spark://master:7077 -m 512m -c 1
zeppelin-daemon.sh start
jps

마스터 > 설정 > 네트워크 > 고급 > 포트포워딩 > 추가 > 192.168.10.1	7777	 192.168.10.10	7777

윈도우 > http://192.168.10.1:7777 접속
create new note > 이름 : mynote  만들기
sc		// shift+enter 에러 > spark 버전이 2.1이어야 호환됨(우리는 2.4)

리눅스로 돌아가서 > 
zeppelin-daemon.sh stop
stop-master.sh
stop-slave.sh

spark 2.1 받아 놓은 것 winscp로 리눅스 root에 전송

cd ~
mv spark spark_bak

tar zxvf 압축파일2.1
mv spark-2.1압축 푼 파일 spark
rm 압축파일

다시 윈도우 > http://192.168.10.1:7777 접속
sc
spark 		//실행해보기

--------------------------- 실습환경 설정 완료 --------------------------------







