# 스파크 시작
	1) 설치
			- 프로그램준비
				http://spark.apache.org에서 다운로드
				압축풀기 : tar zxvf spark(tab키)
				설치폴더명은 spark로 변경 : mv spark-xxx spark
				
			- 환경변수 설정
				gedit /etc/profile
				------------------------
				export SPARK_HOME=/root/spark
				export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
				---------------------------
				source /etc/profile
				stop-dfs.sh
				stop-yarn.sh
				reboot
				
				하둡 켜지말고 대기
			
			- 환경변수 및 설치 테스트
				spark-shell 	: scala 언어 사용
					sc	: SparkContext
					spark:	SparkSession
					3+4
					
					ctrl+c로 빠져나옴
				
				pyspark	: python 언어 사용
					sc	
					spark
					3+4
					exit()
					
				spark-sql 	: sql언어 사용
					select 3+4;

	2) StandardAlone 설치
		free -g
		grep -c /proc/cpuinfo	

		cd $SPARK_HOME/conf
		cp spark-env.sh.template spark-env.sh
		
		gedit spark-env.sh
		----------------------------
		export SPARK_WORKER_INSTANCES=3
		----------------------------
		
		start-master.sh
		
		http://localhost:8080 
		http://192.168.10.1:8080 
		
		start-slave.sh spark://master:7077 -m 512m -c 1
		
		jps		//worker 3개 뜨나
		
		stop-slave.sh
		stop-master.sh
		
	3) Zeppelin : 노트북 서비스
		spark : 2.1, zeppelin : 0.7 버전
		
		- 프로그램 다운로드	
			https://zeppelin.apache.org/
			
		- 리눅스로 root폴더로 전송	
			 - 리눅스로 전송 후 압축 풀고 이름은 zeppelin으로 변경
                  tar zxvf zeppelin...
                  mv zeppelin... zeppelin
             
            - 환경변수 설정
                  gedit /etc/profile
                  --------------------
                  export ZEPPELIN_HOME=/root/zeppelin
                  export PATH=$PATH:$ZEPPELIN_HOME/bin
                  --------------------
                  source /etc/profile
                  reboot
				  
			- zeppelin 환경 설정
				cd $ZEPPELIN_HOME/conf
				cp zeppelin-site.xml.template zeppelin-site.xml
				
				gedit zeppelin-site.xml
				---------------------------
					서버 ip와 포트번호 설정
				
				cp zeppelin-env.sh.template zeppelin-env.sh				
				gedit zeppelin-env.sh	
				--------------------------
					export JAVA_HOME=$JAVA_HOME
					export MASTER=spark://master:7077
					export SPARK_HOME=/root/spark
					
			- 서버 실행
				start-master.sh
				start-slave.sh spark://master:7077 -m 512m -c 1
				zeppelin-daemon.sh start
				jps
			
	4) 클라우드 사용법	
		- Databricks
			https://community.cloud.databricks.com
			
		- MS Asure HDinsight
		
	- 실습환경 설정 완료 -	
		
# 스파크 데이터 관리

	[0925_수업]
	# spark-shell
	
		scala> sc
		
		scala> spark
		
		scala> 3+4
		
		Ctrl + C
	
	# pyspark
	
		>>> sc
		
		>>> spark
		
		>>> 3+4
		
		>>> exit()
	
	# java -version
	
	# spark-sql
		spark-sql> select 3+4;
		
		Ctrl + C
		
	# start-master.sh
	
	# start-slave.sh spark://master:7077 -m 512m -c 1
	
	# jps
	
어제 복습
=====================================

3) Zeppelin : 노트북 서비스
	spark : 2.1, zepplelin : 0.7
	
	- 프로그램 다운로드
		zepplelin : 0.7
			> https://zeppelin.apache.org/
			> Download 클릭
			> Old releases 
				> zeppelin-0.7.3-bin-all.tgz
			> https://archive.apache.org/dist/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-all.tgz
		
		spark : 2.1
			> https://spark.apache.org/downloads.html
			> Archived Releases
				> Spark release archives
				 > spark-2.1.3/
				 > spark-2.1.3-bin-hadoop2.7.tgz 
	
	- 리눅스로 전송후 압출풀고 이름은 zeppelin으로 변경[zeppelin-0.7.3-bin-all.tgz]
		# tar zxvf zepp..
		# mv zepp~ zeppelin
		# rm zeppel~
		
	- 환경변수 설정
		# gedit /etc/profile
		-------------------
		export ZEPPELIN_HOME=/root/zeppelin
		export PATH=$PATH:$ZEPPELIN_HOME/bin
		-------------------
		# source /etc/profile
		# reboot
		
	- zeppelin 환경 설정
		# cd $ZEPPELIN_HOME/conf
		# cp zeppelin-site.xml.template zeppelin-site.xml
		# gedit zeppelin-site.xml
			// ifconfig로 ip 확인(192.168.10.10)
		--------------------------------------
		<name>zeppelin.server.addr</name>
			<value>0.0.0.0</value> -> <value>192.168.10.10</value>
			
		<name>zeppelin.server.port</name>
			<value>8080</value> -> <value>7777</value>
		--------------------------------------	
		
		# cp zeppelin-env.sh.template zeppelin-env.sh
		# gedit zeppelin-env.sh
		-------------------------
		맨밑에
		export JAVA_HOME=$JAVA_HOME
		export MASTER=spark://master:7077
		export SPARK_HOME=/root/spark
		---------------------------
	
	- 서버 실행
		# start-master.sh
		# start-slave.sh spark://master:7077 -m 512m -c 1
		# zeppelin-daemon.sh start
		
		# jps
			> master, worker 3개, ZeppelinServer
			
		---------------------
		virtual box
		> master 오른쪽 마우스 클릭
			> 설정
			> 네트워크
			> 고급
			> 포트 포워딩
			> 추가
				> 호스트 IP : 192.168.10.1
				> 호스트 포드 : 7777
				> 게스트 IP : 192.168.10.10
				> 게스트 포트 : 7777
				
	- window 서버에서 http://192.168.10.1:7777
		// zeppelin서버 뜬다
		
		Notebook > create new notebook
			> Name : mynote
			> Default Interpreter : spark
			
			# sc
				// 버전 충돌땜에 zeppelin은 실행이 된느데 spark실행 안됨
				
		-----------------------------
		터미널로 이동
		=========
		# zeppelin-daemon.sh stop
		# stop-slave.sh
		# stop-master.sh
		
		WinSCP [spark-2.1.3-bin-hadoop2.7.tgz] 리눅스로 이동
		# cd ~
		# mv spark spark_bak
			// 기존 spark 백업으로 이름 바꾸기
		
		# tar zxvf spark-2.1.3-bin-hadoop2.7.tgz
		
		# mv spark-2.1.3-bin-hadoop2.7 spark
		 
		# rm spark-2.1.3-bin-hadoop2.7.tgz
		
		# start master.sh
		# start-slave.sh spark://master:7077 -m 512m -c 1
		# zeppelin-daemon.sh start
		
		# jps
			// master, worker, zeppelinserver
		--------------------------------------
		http://192.168.10.1:7777 이동
		> 아까 썼던 노트 열기
		
		# sc
			// SparkContext
		
		# spark
			// SparkSession
		
				 
4) 클라우드 사용법
	- Databricks
		http://community.cloud.databricks.com
		> 가입
		> COMMUNITY EDITION > Get Started
		
		왼쪽 상단 Clusters (2시간 뒤에 서버 꺼짐)
		> Create Cluster
			> Cluster Name : mycluster
			> Databricks Runtime Version : 7.2(Scala 2.12, Spark 3.0.0)
			> 기본 설정 있는 그대로
			
			> Create Cluster 클릭
			
		Main으로 갔다가 다시 Cluster클릭 해보면 칸이 하나 생김
		
		> mycluster 클릭
			> 상단에 Libraries 클릭
			> Install now
				> Maven
					> Search Packages
					> graphframes > select
				> install
				
			> Install now
				> PyPI
					> pandas
				> install
		
		// 왼쪽의 WorkSpace에 가면 이전에 작업한것 볼 수 있다
		Main으로 이동
		> New Notebook
			> Name : mynote
			> Default Language : Python
			> Cluster : mycluster
			
			# sc  // (Shift + Enter)
				// SparkContext 나옴
				
			# spark
				// SparkSession
		
			# value = "spark"
			  print("이것은 {}".format(value))
				// 이것은 spark
		
		> 위에 attached cluster 
			> mycluster
			> confirm 클릭
		
	- MS Asure HDinsight

---------------------
<<< 0928 수업 >>>		
(8) 스파크 데이터 관리
	1) RDD
		- 저수준 API
		- Lazy Evaluation
			- Transformation
				map(), filter(), flatmap(), sample(), groupByKey(), reduceByKey(), sort(), ....
				
			- Action
				count(), collect(), reduce(), lookup(), save(), ...
				
		- 함수 위주의 연산
		- schemaless
	--------------------------------------
	Documenmts
		spark.apache.org 접속
			> Documents > Latest release
			> Programming Guide > RDD
	--------------------------------------
	http://community.cloud.databricks.com 접속 (비밀번호 대문자+소문자+특수문자)
		> Cluster
			> Create cluster
			> name : mycluster
			
		> Workspace
			> 이전에 만들어놓은 mynote
			> Detached 라고 되어있는거  cluster에 attach시켜줘야한다
				// 새로 만들면 상관 없는데 기존에 있는거 불러올때는 꼭 진행
	------------------------------------------------------
		%python
		%scala	// scala로 지정한다는 걸 명시  
		
		val a = List("spark", "rdd", "example", "sample", "example")
		a
		>> a: List[String] = List(spark, rdd, example, sample, example)
		   List[String] = List(spark, rdd, example, sample, example)
		
		%scala
		val b = sc.parallelize(List("spark", "rdd", "example", "sample", "example"), 3)
		b
		b.count()
		>> b: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at command-4188181027697529:1
		   Long = 5
		   
		%scala
		val c = sc.makeRDD(List("spark", "rdd", "example", "sample", "example"), 3)
		c
		c.collect()
		>> c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at makeRDD at command-4188181027697530:1
		   Array[String] = Array(spark, rdd, example, sample, example)
		   
		%scala
		val d = sc.makeRDD(0 to 1000000000) // 작동 시간 짧음
		d.count()	// 작동 시간이 늘어남
		
		%scala
		// val e = b.map(b => (b, 1))
		// val e = b.map((_, 1))
		val e = b.map(b => (b, b.length))
		e.collect()
		>> Array[(String, Int)] = Array((spark,1), (rdd,1), (example,1), (sample,1), (example,1))
		
		%scala
		val nums = sc.parallelize(List(2, 1, 4, 3))
		nums.max()
		nums.min()
		nums.sum()
		>> Int = 4
		   Int = 1
		   Double = 10.0
		   
		%scala
		val number = sc.parallelize(10 to 50 by 10)
		number.take(5)
		>> Array[Int] = Array(10, 20, 30, 40, 50)
		
		%scala
		val numsq = number.map(num => num * num)
		numsq.take(5)
		>> Array[Int] = Array(100, 400, 900, 1600, 2500)
		
		%scala
		val numrv = numsq.map(num => num.toString.reverse)
		numrv.take(5)
		>> Array[String] = Array(001, 004, 009, 0061, 0052)

		- wordcount
			%scala
			
			val data = sc.textFile("/FileStore/tables/RandomString.txt")
			data.take(100)
			
			val datakv = data.map(word => (word, 1))
			datakv.take(10)
			
			val dataresult = datakv.reduceByKey(_ + _)
			dataresult.take(10)
			
			dataresult.saveAsTextFile("/FileStore/tables/RandomStringResult.txt")
			
			dataresult.saveAsTextFile("/RandomStringResult")

			display(dbutils.fs.ls("dbfs:/RandomStringResult"))
			
			
		> 왼쪽 Data 클릭
			> browse
			> C:\BigData\hadoop-2.10.0\README.txt 파일 클릭
		--------------------------------------------------------------------
		%python
		
		text_file = sc.textFile("/FileStore/tables/RandomString.txt")
		counts = text_file.map(lambda word : (word, 1).reduceByKey(lambda a, b : a+b))
		counts.take(10)
		
		counts.saveAsTextFile("/root/sparkex/RandomStringResult.txt")
		
		> master에서 해보기
		   
	2) DataFrame
		- 고수준 API
		- Schema 존재
		- UDF를 지원
		- RDD를 DataFrame으로 변환
			튜플
			case case
			createDataFrame()
			--------------------------------------------------
			italianPosts.csv
			=================
			• commentCount: 이 포스트에 달린 댓글 개수
			• lastActivityDate: 마지막 수정 날짜 및 시각
			• ownerUserId: 포스트를 게시한 사용자 ID
			• body: 질문 및 답변 내용
			• score: ‘좋아요’와 ‘싫어요’ 개수로 계산한 총 점수
			• creationDate: 생성 날짜 및 시각
			• viewCount: 조회 수
			• title: 질문 제목
			• tags: 질문에 달린 태그들
			• answerCount: 질문에 달린 답변 개수
			• acceptedAnswerId: 답변이 채택된 경우 채택된 답변의 ID
			• postTypeId: 이 포스트의 유형. 1: 질문, 2: 답변
			
			mynote에서 계속
			--------------------------------------------------
			a) 튜플을 이용한 방법
				%scala
				val data = sc.textFile("/FileStore/tables/italianPosts.csv")
				data.take(10)
				
				val datasplit = data.map(_.split("~"))
				datasplit.take(10)
				
				val datardd = datasplit.map(x => (x(0), x(1), x(2), 
												  x(3), x(4), x(5),
												  x(6), x(7), x(8),
												  x(9), x(10), x(11), x(12)))
				datardd.take(10)
				
				val df = datardd.toDF()
				df.show

				df.printSchema()

				val datadf = datardd.toDF("commentCount", "lastActivityDate", "ownerUserID", "body", "score", "creationDate", "viewCount", "title", "tags", "answerCount", "acceptedAnswerId", "postTypeId", "id")
				datadf.show
				
				>> mycluster 이름으로 cluster새로 추가
			
			<< 0929 수업 >>
			b) case class를 이용하는 방법
				
				%scala
				case class Post(
					commentCount:Option[Int],
					lastActivityDate:Option[java.sql.Timestamp],
					ownerUserID:Option[Long],
					body:String,
					score:Option[Int],
					createDate:Option[java.sql.Timestamp],
					viewCount:Option[Int],
					title:String,
					tags:String,
					answerCount:Option[Int],
					acceptedAnswerId:Option[Long],
					postTypeId:Option[Long],
					id:Long
				)
				
				object StringImplicits {
					implicit class StringImpovements(val s:String) {
						import scala.util.control.Exception.catching
						
						def toIntSafe = catching(classOf[NumberFormatException]) opt s.toInt
						def toLongSafe = catching(classOf[NumberFormatException]) opt s.toLong
						def toTimeStampSafe = catching(classOf[IllegalArgumentException]) opt java.sql.Timestamp.valueOf(s)
					}
				}

				import StringImplicits._

				def StringToPost(row:String):Post = {
					val r = row.split("~")
					Post(r(0).toIntSafe,
						r(1).toTimeStampSafe,
						r(2).toLongSafe,
						r(3),
						r(4).toIntSafe,
						r(5).toTimeStampSafe,
						r(6).toIntSafe,
						r(7),
						r(8),
						r(9).toIntSafe,
						r(10).toLongSafe,
						r(11).toLongSafe,
						r(12).toLong
					)
				}
				
				val data = sc.textFile("/FileStore/tables/italianPosts.csv")
				val stringtopost = data.map(x => StringToPost(x)).toDF()
				stringtopost.show
				
				
				%scala
				stringtopost.printSchema
				-------------------------------------------------------------
				
				AWS에 접속
				> 이름 누르면 밑에 '내 보안 자격증명' 클릭
					> 엑세스 키 (AKIAIXTSG6BHMI2C4HOA)
					> 없는 경우 만들기 눌러서 새로 만들면 된다
					> 파일 다운 받아서 키랑 비밀번호 확인
				
				DataFrameTest (notebook새로 추가)
				- 항공운항 데이터로 실습
				AWSAccessKeyId=AKIAIEESOGGMZJC2QCAA
				AWSSecretKey=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
				------------------------------------------------------------
				ACCESS_KEY_ID = "AKIAIXTSG6BHMI2C4HOA" /Key
				SECRET_ACCESS_KEY = "Y6TyL7Mi+E+ZzR4pLcYpHDSOSynD2ozJMaEsXGfk" /비밀번호
				ENCODED_SECRET_ACCESS_KEY = SECRET_ACCESS_KEY.replace("/", "%2F")
					
				dbutils.fs.mount("s3a://%s:%s@edwith-pyspark-dataset" % (ACCESS_KEY_ID, ENCODED_SECRET_ACCESS_KEY), "/mnt/us-carrier-dataset")
				>>  True // 22년의 자료를 사용할 수 있다.
				
				display(dbutils.fs.ls("/mnt/us-carrier-dataset"))
				
				raw_df = spark.read.csv("/mnt/us-carrier-dataset", header="True", inferSchema="True")
				
				raw_df.printSchema()
				
				raw_df.count()
				
				raw_df.show(30)
				
				display(raw_df)
				
				-----------------------------------------------------------
				a) 전처리
					- 사용하지 않을 column 삭제
					- null값 처리
					- 데이터 타입 변경
					
				----------------------------------------------------------
				# 데아터 전처리
				##############
				from pyspark.sql.functions import udf
				from pyspark.sql.types import BooleanType, IntegerType, StringType
				
				# "NA"값을 null로 변경
				def cleanString(value):
				  if(value == None) or (value == "NA"):
					return None
				  else:
					return value
					
				# string으로 설정된 숫자를 Integer로 변경
				def stringToInteger(value):
				  if value == "NA":
					return None
				  else:
					return int(value)
					
				# Integer값을 Boolean으로 변경
				def integerToBoolean(value):
				  return False if value == 0 else True
				  
				cleanStrFunction = udf(stringToInteger, IntegerType())
				cleanIntegerFunction = udf(integerToBoolean, BooleanType())
				cleanStringFunction = udf(cleanString, StringType())
								 
				us_carrier_df \
				  = raw_df.drop('DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'AirTime', 'ArrDelay', 'DepDelay', 'TaxiIn', 'TaxiOut', 'CancellationCode', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay')\
					  .withColumn("ActualElapsedTime", cleanStrFunction("ActualElapsedTime"))\
					  .withColumn("CRSElapsedTime", cleanStrFunction("CRSElapsedTime"))\
					  .withColumn("Distance", cleanStrFunction("Distance"))\
					  .withColumn("Cancelled", cleanIntegerFunction("Cancelled"))\
					  .withColumn("Diverted", cleanIntegerFunction("Diverted"))
					  
				
				us_carrier_df.printSchema()
				
				us_carrier_df.cache()
				
				us_carrier_df.createGlobalTempView("us_carrier")
				
				
				# 얼마나 많은 항공사가 있을까?

				carriers_only_df = us_carrier_df.select("UniqueCarrier")
				carriers_only_df.show(50)
				
				carriers_only_distinct_df = carriers_only_df.distinct()
				carriers_only_distinct_df.show()
				
				spark.sql("SELECT DISTINCT UniqueCarrier FROM global_temp.us_carrier").show()
				// SQL로 작성하는 방법
				
				-----
				
				# DL 항공사는 1990년에 얼마나 비행을 했는가?
				from pyspark.sql.functions import col

				us_carrier_df.filter(col("UniqueCarrier") == "DL").show()
				us_carrier_df.filter(col("Year") == 1990).show()

				## 위에 두개 한줄로 합한 것
				us_carrier_df.filter(col("UniqueCarrier") == "DL").filter(col("Year") == 1990).show()

				us_carrier_df.filter(col("UniqueCarrier") == "DL") & (col("Year") == 1990).show()

				## sql 문으로 작성
				spark.sql("SELECT * FROM global_temp.us_carrier WHERE UniqueCarrier == 'DL' AND Year == 1990").show()
				
				----
					
	3) DataSet
		- 고수준 API
		- java, scalar

(9) 스파크 사용 방법
	1) 대화형
	2) 응용 프로그램
		spark-submit --master local[4] SampleApp.py
		spark-submit --master spark://master:7077 SampleApp.py
		(둘 중 하나로 실행가능)
		
		sc 객체 생성 (메모장 파일 맨 윗줄에 추가)
		---------------
		from pyspark import SparkContext, SparkConf
		
		conf = SparkConf().setAppName("Spark Count")
		sc = SparkContext(conf=conf)
	

				
			


















				